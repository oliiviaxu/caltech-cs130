CS130 Project 2 - Design Document
=================================

Please answer all questions in this design document.  Note that the final
feedback section is optional, and you are not required to answer it if you
don't want to.

Unanswered or incompletely answered questions, or answers that don't actually
match the code/repository, will result in deductions.

Answers don't have to be deeply detailed!  We are mainly looking for an
overview or summary description of how your project works, and your team's
experiences working on this project.

Logistics (7 pts)
-----------------

L1.  [2pts] Enumerate all teammates here.
- Olivia Xu, Wilson Duan

L2.  [2pts] What did each teammate focus on during this project?
Olivia: 
- Saving workbook as json
- Reordering and copying sheets
- Fixing project 1 errors
- Performance analysis tests and profiler

Wilson:
- Loading workbook as json
- Renaming sheet
- Writing correctness tests
- Fixing project 1 errors

L3.  [3pts] Approximately how many hours did each teammate spend on the project?
Olivia: 
- 15

Wilson:
- 15

Spreadsheet Engine Design (11 pts)
----------------------------------

D1.  [3pts] Briefly describe how your workbook-loading code operates.  Does
     it do anything sophisticated to optimize the performance of loading a
     workbook, such as deferring cell-value update calculations, or analyzing
     the graph of cell dependencies?
- Load workbook is not sophisticated at the moment:
    - It loops through all sheets, and for each sheet, it loops through all the
      cell contents and makes a call to set_cell_contents
- This approach has no performance optimizations, although that is some technical
  debt we will incur

D2.  [4pts] Sheet-level operations like copying a sheet, deleting a sheet,
     renaming a sheet, or even creating a new sheet, can cause cell values
     to be updated.  How does your workbook identify such cells and ensure
     that they are updated properly?
- The workbook identifies cells that needs to cause updates through the dependency
  graph, which stores outgoing and ingoing connections in a separate class
- Each operation handles the new cell value updates differently:
    - Copying a sheet/creating a new sheet:
        - After the sheet is created, we look through the dependency graph
          for any ingoing connections with the new sheet name, and propagate
          updates from there
    - Deleting a sheet:
        - When deleting a sheet, we look through all outgoings from this sheet,
          and remove those edges by also removing the ingoings from the outgoing cells
        - In addition, for all ingoing edges to the sheet, we propagate bad reference
          errors by using set_cell_contents to set the contents to be a bad reference
          before we actually delete the cell
    - Renaming a sheet:
        - Similar to deleting a sheet, we update the ingoing edges of all the outgoing
          cells and all the outgoing edges of the ingoing cells to reflect the new sheet
          name
        - Once the dependency graph is corrected for the new sheet name, we can propagate
          cell updates by a call to set_cell_contents for each cell with ingoing edges in
          the sheet

D3.  [4pts] When renaming a sheet, cells with formulas that explicitly
     reference the renamed sheet must be updated with the new sheet name.
     Give an overview of how your implementation updates these formulas.
- We first find the cells that reference the renamed sheet through the
  dependency graph's ingoing edges
- For these cells, we used a transformer to change the formula
    - First use the same lark grammar to parse the formula
    - Feed into a transformer, which essentially creates a string copy of
      the formula, with a small exception: when it finds cell references,
      if it finds a cell reference with the old sheet name, it converts it
      to the new sheet name. The transformer also handles other cases related
      to sheet quotes (removing sheet quotes when they are unnecessary).
    - We call set_cell_contents using the new formula from the transformer

Informal Design Reviews (16 pts)
--------------------------------

R1.  [4pts] What insights did your team gain regarding the design of *your
     own* spreadsheet engine code?  What parts of your design are you happy
     with?  What parts might require further attention in the future?
- We learned that our design is very similar to other teams, and we confirmed
  that our approach has intuitive motivations
- We are especially happy with our dependency graph implementation, as it is
  loosely coupled with other aspects of the code
- We will have to tweak our automatic updating of cells due to performance issues
- Our cell detection may also have to change slightly due to performance issues
- We may consider implementing a CellValue class once there are a larger variety
  of cell types such as Dates

R2.  [4pts] Did you feel like you were effective at helping other teams
     assess *their* software designs?  Briefly discuss what went well, and
     what could have gone better, in your interview of another team.
- We believed we were effective at helping the other team identify issues with
  the way they handled the updating of cells after new_sheet, del_sheet, rename_sheet,
  copy_sheet
    - They had an inefficient way of reupdating the cells that we helped them diagnose
- What went well:
    - Having a list of important questions to ask was helpful, it allowed for a
      guided and informative discussion
- What could have gone better:
    - When the other team had a different way of implementing some aspect of the project,
      it was sometimes difficult to understand
        - We somewhat mitigated this problem by asking how their code solves a specific
          example

R3.  [4pts] How closely did your team's design match the designs of the
     other teams you talked with?  Briefly discuss significant similaries
     and differences between your team's approach and other teams' approaches.
- Our design shared some traits with all teams: having a Sheet class and Cell class,
  not having a CellValue class
- Our design was almost identical to the team that interviewed us, which made us happy
  because it validated lots of our design choices
    - We had a dependency graph class similar to theirs (map for ingoing and outgoing edges)
- The team that we interviewed implemented a dependency graph exactly how we implemented
  it before - we decided to refactor it to avoid inefficient updating of cells upon sheet
  operations
    - They said they were incurring technical debt by not refactoring their code,
      as they have an inefficient way of handling updating of cells in some scenarios

R4.  [4pts] Which GRASP principles were the most pertinent in your
     discussions?  How much of your discussions referenced the GRASP
     principles?
- The most pertinent principle was coupling - we discussed the coupling of the dependency
  graph in both interviews
    - We agreed that more loosely coupled dependency graph made it more robust and effective
- We also agreed on high cohesion - a Sheet class and Cell class contributed
  to this cohesion
- With one of our groups, we agreed that the Workbook class had the most code due to the
  principle of information expert - the Workbook has information about all the sheets
  and cells, making it the ideal location for many operations

Performance Analysis (16 pts)
-----------------------------

In this project you must measure and analyze the performance of two central
areas of your spreadsheet engine.  Using pair programming, construct some
performance tests to exercise these aspects of your engine, and use a profiler
to identify where your program is spending the bulk of its time.

A1.  [4pts] Briefly enumerate the performance tests you created to exercise
     your implementation, along with the teammates that collaborated to
     implement each of them.
     - test large cycles (Wilson, Olivia)
     - test small cycles (Wilson, Olivia)
     - test long chain 
     - testing many references (one cell has many references)
     - test rename sheet operations (using a long chain between two sheets)
     - test cycle making and breaking
     - test circular references

     We organized our test suite in a way that separates unit and integration
     tests. Those enumerated above are found in tests/performance directory. The
     performance tests are further grouped into different classes for testing 
     updates, cycles, and general performance. 
     
A2.  [2pts] What profiler did you choose to run your performance tests with?
     Why?  Give an example of how to invoke one of your tests with the profiler.

    - We chose to use the cProfile module for our performance tests. It was 
    recommended in the spec and we felt that it was well-documented and has 
    good support from other Python packages and tools, such as pstats for analyzing 
    the output and SnakeViz for visualizing the call graph that we want to
    use in the future.
    
    We use the setUp and tearDown methods in our unittest.TestCase classes to 
    ensure that profiling is set up and executed for each test method automatically. 
    This allows us to profile all test methods (those enumerated above in A1) 
    without adding any explicit profiling code within each test.

    To invoke the CycleDetectionTests with the profiler, we use the following 
    command in the terminal:

    (Bash)

    python -m unittest tests.performance.test_cycles

    This command runs the cProfile profiler on the test_cycles module, generating 
    profiling data for each test method within that module. The output is saved 
    in a file, which is then processed and sorted by cumulative time (cumtime) 
    using pstats library. This sorting helps us quickly identify the most time-consuming 
    functions and potential bottlenecks in our code.

A3.  [6pts] What are ~3 of the most significant hot-spots you identified in your
     performance testing?  Did you expect these hot-spots, or were they
     surprising to you?
     Through our performance testing using cProfile, we identified these hotspots:

     - set_cell_contents: This function consistently appeared as a major contributor 
     to execution time in our tests. This was expected, as set_cell_contents involves 
     updating both the cell's content and the dependency graph, which can be 
     computationally expensive, especially when dealing with large chains or webs 
     of cell dependencies.

     - handle_update_tree: This function emerged as a significant hot-spot, 
     particularly in tests involving long chains of dependencies. This was not 
     surprising, as handle_update_tree is responsible for updating all dependent 
     cells and recalculating their values, which can be time-consuming when a single 
     update triggers a cascade of changes across many cells.

    - evaluate_cell: This function showed up as a hot-spot in tests where we created 
    long chains as well as when many cells depend on a cell. This is expected because 
    evaluate_cell iterates through all cell references in a formula, and when many 
    cells rely on the same cell, evaluating those cells can become computationally 
    intensive. 


A4.  [4pts] Reflect on the experience of pair-programming as you constructed
     these tests.  What went well with it?  What would you like to try to do
     better in the future?
     - Initially, I (Olivia) focused on designing complex and obscure tests. 
     However, Wilson provided the insightful suggestion to simplify the tests, 
     emphasizing that simplicity doesn't compromise their ability to uncover 
     code vulnerabilities. This collaborative exchange led to more effective 
     test designs. 
     - Wilson pointed out areas where my tests were not consistent, or where
     the tests themselves could be optimized. For instance, in some assert
     statements I would call detect_cycle, where instead it was
     more concise and accurate to check the value of the cells in a cycle and
     ensure that they were set to CIRCREF. 
     - In the future, we aim to expand our performance testing to 
     include a wider range of spreadsheet functionalities, such as deleting 
     sheets, to ensure comprehensive performance evaluation across the application. 
     Additionally, we plan to explore more advanced profiling techniques 
     and tools to gain deeper insights into performance bottlenecks and optimize 
     our code further.


Section F:  CS130 Project 2 Feedback [OPTIONAL]
-----------------------------------------------

These questions are OPTIONAL, and you do not need to answer them.  Your grade
will not be affected by answering or not answering them.  Also, your grade will
not be affected by negative feedback - we want to know what went poorly so that
we can improve future versions of the course.

F1.  What parts of the assignment did you find highly enjoyable?  Conversely,
     what parts of the assignment did you find unenjoyable?


F2.  What parts of the assignment helped you learn more about software
     engineering best-practices, or other useful development skills?
     What parts were not helpful in learning these skills?


F3.  Were there any parts of the assignment that seemed _unnecessarily_ tedious?
     (Some parts of software development are always tedious, of course.)


F4.  Do you have any feedback and/or constructive criticism about how this
     project can be made better in future iterations of CS130?